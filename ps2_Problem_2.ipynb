{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Problem 2.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "kvKex6ymPBuv",
        "h5JCyk32sqnt",
        "Y4QpEe6lRby9",
        "M9QtfnTJYbkz",
        "R29r-mAIpML2",
        "8yuigGKFV9DG",
        "E638a6_qo4gJ",
        "VTG7ICybrbYx"
      ],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/thotran2015/6.871/blob/master/ps2_Problem_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "loZ-Kj3qPDtb",
        "colab_type": "text"
      },
      "source": [
        "# Problem 2: Clinical NLP for De-identification\n",
        "\n",
        "## Task\n",
        "As outlined in the problem overview, in this problem we will be working with the n2c2 2014 challenge, de-identification track data. This challenge tasks participants to design models to ingest clinical notes and identify regions of Personal Health Information (PHI), so that they can be automatically obfuscated by de-identification systems. For example, if a note said \n",
        "\n",
        "  > Dr. Pompies perscribed Deniz Aslan a daily antibiotic after a visit to The Hospital for Doctoring\n",
        "\n",
        "then the system should flag \"Pompies\", \"Deniz Aslan\" as names for obfuscation, and \"The Hospital for Doctoring\" as the name of a hospital for obfuscation. We work with these labels via a variant of the [IOB2](https://en.wikipedia.org/wiki/Inside%E2%80%93outside%E2%80%93beginning_(tagging)) system, where we tag the _beginnings_ of spans of PHI with a \"B-\" prefixed label (the label suffix indicates what kind of PHI the span is), the _inside_ of spans of PHI with an \"I-\" tag, and all non-PHI tokens (the _outside_ of spans of PHI) with an \"O\" label. This format is commonly used in clinical NER tasks. The labels for our example sentence above would thus be\n",
        "\n",
        "  > O B-DOCTOR O B-PATIENT I-PATIENT O O O O O O O B-HOSPITAL I-HOSPITAL I-HOSPITAL I-HOSPITAL\n",
        "\n",
        "Note that while the data we use in this problem does come from the n2c2 task, for reasons of technical expedience we don't evaluate in a manner consistent with the official challenge evaluation,so your results will not be consistent with published numbers.\n",
        "\n",
        "### Outline\n",
        "In this notebook, first, we'll have some imports and helper functions, then we'll work through some data exploration and baseline methods, followed by methods using recurrent neural networks to acheive better performance.\n",
        "\n",
        "## Import, constants, and helper functions\n",
        "Do not modify this section. Note that after exectuing the first cell, you will need to restart the runtime for the package installations to take effect. Additionally, after you complete this initial restart, make sure you upload the provided data files by clicking on the folder icon on the left, then clicking \"Upload\", then selecting the provided data files. Do not navigate to other folders than the default folder. If you accidentally navigate out to the full file tree, the default folder is `content`.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qkg3HaRpvjSq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install --force https://github.com/chengs/tqdm/archive/colab.zip\n",
        "!pip install allennlp\n",
        "\n",
        "import pandas as pd, numpy as np, matplotlib.pyplot as plt\n",
        "\n",
        "from collections import Counter\n",
        "from itertools import chain\n",
        "from scipy.sparse import csr_matrix, hstack\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from tqdm import tqdm_notebook as tqdm"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kDBvUlhW0mby",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "RARE_WORDS_THRESHOLD = 4\n",
        "UNK = \" _ UNK _ \"\n",
        "PAD = \" _ PAD _ \"\n",
        "PUNCT = \"PUNCTUATION\"\n",
        "NUM = \"#\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "duq4QV0o2Rf1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def read_dataset_file(filepath):\n",
        "  with open(filepath, mode='r') as f: lines = f.readlines()\n",
        "  out, sentence = [], []\n",
        "  for line in lines:\n",
        "    if not line.strip(): \n",
        "      out.append(sentence)\n",
        "      sentence = []\n",
        "    else: \n",
        "      word, label = line.strip().split()\n",
        "      sentence.append((word, label))\n",
        "  return out\n",
        "\n",
        "train = read_dataset_file('2014_train.tsv')\n",
        "dev = read_dataset_file('2014_dev.tsv')\n",
        "test = read_dataset_file('2014_test.tsv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GxvoCoeUvVX2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def ner_eval(\n",
        "    probs, true, label_vocab, metric='Macro F1',\n",
        "):\n",
        "  \"\"\"\n",
        "  Simple, *NON-OFFICIAL* evaluation script. \n",
        "  probs: [ sentence 1: [[probabilities_per_label for word 1], ... ], ...]\n",
        "  true: [ sentence 1: [(word_1, label_1), ...], ...]\n",
        "  label_vocab: [labels in order of elements of probability vectors.]\n",
        "  metric = {'F1', ...}\n",
        "  \"\"\"\n",
        "  label_idxmap = {l: i for i, l in enumerate(label_vocab)}\n",
        "\n",
        "  assert metric == 'Macro F1', \"Metric %s not supported\" % metric\n",
        "\n",
        "  if metric == 'Macro F1':\n",
        "    flat_preds = [probs.argmax() for sent in probs for probs in sent]\n",
        "    flat_true = [label_idxmap[\n",
        "        l if l in label_idxmap else PAD\n",
        "    ] for sent in true for _, l in sent]\n",
        "    score = f1_score(flat_true, flat_preds, average='macro')\n",
        "  \n",
        "  return score"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kvKex6ymPBuv",
        "colab_type": "text"
      },
      "source": [
        "## Dataset Exploration\n",
        "Now that we've loaded our data, let's take a look at its format.\n",
        "\n",
        "### Examining the Data Directly\n",
        "What can we see here about this data? We can observe both technical properties, about how the data is organized, and conceptual factors, such as the kinds of labels present in the data.\n",
        "\n",
        "#### Technical Factors\n",
        "Let's print some stats about our dataset, like its size, and show some examples to see if we can understand the format."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JrKHptKXtagb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_len = len(train)\n",
        "dev_len = len(dev)\n",
        "test_len = len(test)\n",
        "\n",
        "train_lens = [len(s) for s in train]\n",
        "dev_lens = [len(s) for s in dev]\n",
        "\n",
        "print(\n",
        "    \"Train contains %d sentences, ranging from %d - %d (avg %d) words long.\"\n",
        "    \"\" % (train_len, min(train_lens), max(train_lens), np.mean(train_lens))\n",
        ")\n",
        "print(\"Train Samples:\")\n",
        "print(train[0])\n",
        "print(train[1])\n",
        "\n",
        "print(\n",
        "    \"Dev contains %d sentences, ranging from %d - %d (avg %d) words long.\"\n",
        "    \"\" % (dev_len, min(dev_lens), max(dev_lens), np.mean(dev_lens))\n",
        ")\n",
        "print(\"Test contains %d sentences.\" % test_len)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h5JCyk32sqnt",
        "colab_type": "text"
      },
      "source": [
        "It appears our data-loaders have loaded the files in such a way that `train` or `dev` contains a list, where each element of the list corresponds to a sentence or line in a clinical note, represented as a list of tuples, each containing first the _word_ at that position in the sentence, followed by the _label_ of that word. Note that the underlying file format we use, if you choose to inspect that, is a common format for use with the BERT model and NER (though we won't use BERT in this assignment).\n",
        "\n",
        "Note the size discrepancy between our dev dataset and test dataset. In general, seeing such a large difference would be cause for concern -- perhaps these were not sampled in an iid manner, but instead obtained from different sources and this explains the delta in size. Here, however, it is merely that the original source data for this task used a 60/40 train-test split, which is abnormally large, and our selected dev set (randomly chosen from the original data) is only a 10% split. See here: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4989908/ for more details\n",
        "\n",
        "#### Labels present\n",
        "Stripping off the IOB format prefixes (`B-`, `I-`), we see that we have 24 PHI categories, including non-PHI (`O`). Normally, you'd need to investigate the data generative process, or speak to your clinical collaborators to understand the exact meaning of these labels, but in this case, you can refer to the published paper describing this dataset for more information on the meaning of each of these labels: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4989908/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QMq8RfUE4duM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_labels = set(l for sent in train for w, l in sent)\n",
        "# Note that, in reality, we won't be able to use the labels we add below from\n",
        "# dev in any model decisions if it is to truly simulate a held out test set.\n",
        "dev_labels = set(l for sent in dev for w, l in sent)\n",
        "test_labels = set(l for sent in read_dataset_file('2014_test.tsv') for w, l in sent)\n",
        "\n",
        "print(dev_labels - train_labels)\n",
        "print(test_labels - train_labels)\n",
        "\n",
        "all_labels = train_labels\n",
        "all_labels.update(dev_labels)\n",
        "all_labels.update(test_labels)\n",
        "\n",
        "phi_categories = set(l if l == 'O' else l[2:] for l in all_labels)\n",
        "phi_category_counts = Counter(\n",
        "    (l if l == 'O' else l[2:]) for sent in train for _, l in sent\n",
        ")\n",
        "print(\n",
        "    \"All %d PHI Types: \\n\"\n",
        "    \"%s\"\n",
        "    \"\" % (\n",
        "        len(phi_categories),\n",
        "        '\\n'.join('  %s: %d' % e for e in phi_category_counts.most_common())\n",
        "    )\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y4QpEe6lRby9",
        "colab_type": "text"
      },
      "source": [
        "### Word Frequencies\n",
        "Now, let's examine the words observed"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sZawHub7vVQl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "words_and_labels = Counter(w_and_l for sent in train for w_and_l in sent)\n",
        "vocab = Counter(w for sent in train for w, l in sent)\n",
        "labels = Counter(l for sent in train for w, l in sent)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wqkJzX0Z0gMa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "rare_words = Counter({w: cnt for w, cnt in vocab.items() if cnt < RARE_WORDS_THRESHOLD})\n",
        "rest_words = Counter({w: cnt for w, cnt in vocab.items() if cnt >= RARE_WORDS_THRESHOLD})\n",
        "\n",
        "rare_words_aggregated = {\n",
        "    l: sum(words_and_labels[(w, l)] for w in rare_words) for l in labels\n",
        "}\n",
        "\n",
        "print(\n",
        "    \"Extracted %d rare words of %d total (%.2f%%)\" % (\n",
        "        len(rare_words), len(vocab), len(rare_words)/len(vocab) * 100\n",
        "    )\n",
        ")\n",
        "print(\"Sample Rare words\", list(rare_words.items())[:5])\n",
        "print(\"Sample Common words\", list(rest_words.items())[:5])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GlrFlJoKWhxs",
        "colab_type": "text"
      },
      "source": [
        "That's a lot of rare words. This could cause problems in our analysis later, and may indicate issues with our (currently extremely simple) tokenization."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qYGMuxq0vXcX",
        "colab_type": "text"
      },
      "source": [
        "## Baselines\n",
        "### A Simple, Unigram Baseline\n",
        "First, we'll experiment with a simple, per-word baseline. Normally, we might consider making a linear model with a unigram, bag-of-words baseline as our simplest baseline. However, some consideration points out that a linear model with _unigram_ inputs is simply asking the model to assign optimal probabilities to each word indepedently, and these we can compute more efficiently, so we'll do that here as our simplest baseline. All we need to do is count how often each label is assigned to any unique word in our train set (aggregating together all rare words into an \"UNKNOWN\" bucket so we can still predict on unseen words at test time), then normalize by word to get probabilities. Note that if you change the `RARE_WORDS_THRESHOLD` variable, this cell may take a long time to compute, but at the default parameters, should take only seconds."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xvG0awTvyBTc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%time\n",
        "all_data = [(w,l,cnt) for (w,l), cnt in words_and_labels.items() if w in rest_words]\n",
        "for label in labels.keys():\n",
        "  all_data.append((UNK, label, rare_words_aggregated[label]))\n",
        "\n",
        "labeling_rates = pd.DataFrame(all_data, columns = ('word', 'label', 'count'))\n",
        "\n",
        "labeling_rates = labeling_rates.pivot_table(\n",
        "    index='word', columns='label', values='count'\n",
        ")\n",
        "labeling_rates.fillna(0, inplace=True)\n",
        "\n",
        "# Convert counts into per-word probabilities\n",
        "labeling_rates = labeling_rates.div(labeling_rates.sum(axis=1), axis='index')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8W_HYn--Xpqe",
        "colab_type": "text"
      },
      "source": [
        "What does our output look like? A table with word as our index key, and each row giving the probabilities for all label types."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1kP6y8iqyHW2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "labeling_rates.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ocGG2IFnWfCB",
        "colab_type": "text"
      },
      "source": [
        "We see that many words are _never_ seen to be any of our \"PHI\" labels here (B-\\*, I-\\*). However, the block in which we've aggregated rare words shows pretty diverse labels, as shown in the plot below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YQz1Vbl2z8H8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fig, ax = plt.subplots(figsize=(7, 7))\n",
        "data = labeling_rates.loc[UNK].sort_values()\n",
        "ax.bar(data.index, data.values)\n",
        "ax.tick_params(rotation=90, axis='x')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w3oQ6668ZTFE",
        "colab_type": "text"
      },
      "source": [
        "Now we can make some actual predictions on the validation dataset and evaluate via our per-word macro F1 metric:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gviqyIFUvVaU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%time\n",
        "dev_probs = []\n",
        "for sent in dev:\n",
        "  sent_probs = []\n",
        "  for w, _ in sent:\n",
        "    w = w if w in rest_words else UNK\n",
        "    probabilities = labeling_rates.loc[w]\n",
        "    sent_probs.append(probabilities.values)\n",
        "  dev_probs.append(sent_probs)\n",
        "\n",
        "unigram_dev_F1 = ner_eval(dev_probs, dev, list(labeling_rates.columns) + [PAD])\n",
        "print(\"We obtain a Dev F1 of %.2f\" % unigram_dev_F1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M9QtfnTJYbkz",
        "colab_type": "text"
      },
      "source": [
        "### An Improved Baseline\n",
        "As far as Macro-F1s go, this isn't great. Note that this isn't a comparable metric to the true SOTA for this task, which uses an exact F1 measure comparing predicted spans, as opposed to our crude, per-word metric here. But still, we can do better.\n",
        "\n",
        "We'll now work through a more involved baseline, which will incorporate 3 primary changes:\n",
        "  1. An improved tokenization of the data.\n",
        "  2. An n-gram model incorporating context words.\n",
        "  3. Additional syntactic features capturing punctuation/stylistic signals that may be indicative of PHI\n",
        "\n",
        "  Note that there are other changes possible that may also yield a much more competitive baseline. Ultimately, your goal in this part of this problem will be to analyze both of these baselines, understand what changes drive the performance discrepancy between them, and how various other settings will affect final performance.\n",
        "\n",
        "#### Re-tokenization\n",
        "Here, we'll write our own tokenizer, though in practice, there are numerous libraries that one _should_ use instead. [spaCy](https://spacy.io/) is a common goto, though for simple baselines, [`scikit-learn`'s Vectorizers](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.feature_extraction.text) have most of the features we include in our tokenizer, plus some extras (stop word removal). We write our own here for 3 reasons:\n",
        "  1. spaCy is heavy, and requires downloading language resource packs, which seemed an unnecessary complication for a homework.\n",
        "  2. `scikit-learn`'s vectorizers are better suited to whole-document or whole-sentence classification, rather than NER, as we do here.\n",
        "  3. We'll use this improved tokenization for our neural network model below as well."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SnbijHpTvVct",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from enum import Enum, auto\n",
        "import copy, string, re, unidecode\n",
        "\n",
        "class AuxiliaryFeatures(Enum):\n",
        "  ENDS_IN_COLON = auto()\n",
        "  IS_CAPITALIZED = auto()\n",
        "  ALL_CAPITALIZED = auto()\n",
        "\n",
        "class Tokenizer():\n",
        "  FEATURE_INDICATORS = {\n",
        "    AuxiliaryFeatures.ENDS_IN_COLON: lambda w: w[-1] == ':',\n",
        "    AuxiliaryFeatures.IS_CAPITALIZED: lambda w: w[0].isupper(),\n",
        "    AuxiliaryFeatures.ALL_CAPITALIZED: lambda w: w.isupper(),\n",
        "  }\n",
        "\n",
        "  def __init__(\n",
        "      self, rare_words_thresh=RARE_WORDS_THRESHOLD,\n",
        "      lowercase=True,\n",
        "      remove_punct=True,\n",
        "      replace_only_punct=PUNCT,\n",
        "      remove_empty_words=True,\n",
        "      replace_numbers=NUM,\n",
        "      auxiliary_features=[\n",
        "        AuxiliaryFeatures.ENDS_IN_COLON, AuxiliaryFeatures.IS_CAPITALIZED\n",
        "      ],\n",
        "      add_pad_label=True,\n",
        "      add_pad_token=True,\n",
        "  ):\n",
        "    self.rare_words_thresh  = rare_words_thresh\n",
        "    self.auxiliary_features = set(auxiliary_features)\n",
        "    self.is_fit = False\n",
        "\n",
        "    self.lowercase          = lowercase\n",
        "    \n",
        "    if remove_punct == True: remove_punct = string.punctuation\n",
        "    self.remove_punct       = remove_punct\n",
        "    self.replace_only_punct = replace_only_punct\n",
        "    self.replace_numbers    = replace_numbers\n",
        "    self.remove_empty_words = remove_empty_words\n",
        "    self.add_pad_label      = add_pad_label\n",
        "    self.add_pad_token      = add_pad_token\n",
        "\n",
        "  def __normalize_dataset(self, dataset):\n",
        "    norm_dataset = []\n",
        "    removed_non_Os = {}\n",
        "    for sent in dataset:\n",
        "      norm_sent = []\n",
        "      for w_orig, l in sent:\n",
        "        w = unidecode.unidecode(w_orig).strip()\n",
        "        if self.lowercase: w = w.lower()\n",
        "        if self.remove_punct:\n",
        "          w = w.translate(str.maketrans('', '', self.remove_punct))\n",
        "          if w_orig and not w and self.replace_only_punct:\n",
        "            w = self.replace_only_punct\n",
        "        if self.replace_numbers:\n",
        "          # This must happen after remove punctuation.\n",
        "          w = re.sub(r'\\d+', NUM, w)\n",
        "\n",
        "        w = w.strip()\n",
        "\n",
        "        if w or (not self.remove_empty_words):\n",
        "          token_feats = []\n",
        "          for aux in self.auxiliary_features:\n",
        "            token_feats.append(float(Tokenizer.FEATURE_INDICATORS[aux](w_orig)))\n",
        "          norm_sent.append(((w, tuple(token_feats)), l))\n",
        "        else:\n",
        "          if l != 'O':\n",
        "            if w_orig not in removed_non_Os:\n",
        "              removed_non_Os[w_orig] = 1\n",
        "              print(\"WARNING: Removing non-'O' token: \\\"%s\\\"!\" % w_orig)\n",
        "            else: removed_non_Os[w_orig] += 1\n",
        "      norm_dataset.append(norm_sent)\n",
        "    return norm_dataset\n",
        "\n",
        "  def __build_vocabs(self, dataset):\n",
        "    # Labels:\n",
        "    self.labels = Counter(l for sent in dataset for w, l in sent)\n",
        "    self.labels_list = [l for l, _ in self.labels.most_common()]\n",
        "    if self.add_pad_label: self.labels_list.append(PAD)\n",
        "\n",
        "    self.labels_idxmap = {l: i for i, l in enumerate(self.labels_list)}\n",
        "    self.num_classes = len(self.labels_list)\n",
        "\n",
        "    # Word Level Vocabs:\n",
        "    # What's our raw, unfiltered vocabulary? Here we just grab the words\n",
        "    # themselves, no auxiliary features.\n",
        "    self.raw_vocab = Counter(w[0] for sent in dataset for w, l in sent)\n",
        "\n",
        "    # Filter out the rare words:\n",
        "    self.rare_words = set(\n",
        "        w for w, cnt in self.raw_vocab.items() if cnt < self.rare_words_thresh\n",
        "    )\n",
        "    self.seen_words = set(\n",
        "        w for w in self.raw_vocab if w not in self.rare_words\n",
        "    )\n",
        "    self.vocab = Counter(\n",
        "        {w: cnt for w, cnt in self.raw_vocab.items() if w in self.seen_words}\n",
        "    )\n",
        "    self.vocab[UNK] = sum(self.raw_vocab[w] for w in self.rare_words)\n",
        "\n",
        "    # Make our idxmaps, for quickly mapping from a word to its numerical index.\n",
        "    self.vocab_list = [w for w, _ in self.vocab.most_common()]\n",
        "    if self.add_pad_token:\n",
        "      self.vocab_list = [PAD] + self.vocab_list\n",
        "      self.vocab[PAD] = 0\n",
        "\n",
        "    self.vocab_idxmap = {w: i for i, w in enumerate(self.vocab_list)}\n",
        "\n",
        "    self.vocab_size = len(self.vocab_list)\n",
        "\n",
        "    # Character Level Vocab:\n",
        "    self.chars = Counter(c for w in self.raw_vocab.keys() for c in w)\n",
        "    self.chars_list = [PAD, UNK] + [c for c, _ in self.chars.most_common()]\n",
        "    self.chars_idxmap = {c: i for i, c in enumerate(self.chars_list)}\n",
        "    self.chars_vocab_size = len(self.chars_list)\n",
        "\n",
        "  def fit(self, dataset):\n",
        "    # transformed_dataset\n",
        "    normalized_dataset = self.__normalize_dataset(copy.copy(dataset))\n",
        "    self.__build_vocabs(normalized_dataset)\n",
        "    self.is_fit = True\n",
        "\n",
        "  def transform(self, dataset, as_index=True):\n",
        "    \"\"\"\n",
        "    dataset is [ (sentence: [(word, label), ...]), ...]\n",
        "    \"\"\"\n",
        "    assert self.is_fit, \"Can't transform without being fit on train data!\"\n",
        "\n",
        "    normalized_dataset = self.__normalize_dataset(copy.copy(dataset))\n",
        "    if not as_index: return normalized_dataset\n",
        "\n",
        "    indexed_dataset = []\n",
        "    for sent in normalized_dataset:\n",
        "      indexed_sent = []\n",
        "      for w, l in sent:\n",
        "        w_idx = self.vocab_idxmap[w[0] if w[0] in self.vocab else UNK]\n",
        "        indexed_sent.append((tuple([w_idx] + list(w[1:])), l))\n",
        "      indexed_dataset.append(indexed_sent)\n",
        "    \n",
        "    return indexed_dataset"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mx7YwONHlapT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokenizer = Tokenizer(rare_words_thresh=10)\n",
        "tokenizer.fit(train)\n",
        "\n",
        "train_tokenized_no_index = tokenizer.transform(train, as_index=False)\n",
        "train_tokenized_index = tokenizer.transform(train, as_index=True)\n",
        "for sent_num in range(2):\n",
        "  print(\"Original (%d)\" % sent_num, train[sent_num][:15], \"...\")\n",
        "  print(\"New (non-indexified)\", train_tokenized_no_index[sent_num][:15], \"...\")\n",
        "  print(\"New (indexified)\", train_tokenized_index[sent_num][:15], \"...\")\n",
        "  print(\"\\n\")\n",
        "\n",
        "print(\n",
        "    \"The new tokenizer contains %d tokens (omitting %d rare):\" % (\n",
        "        tokenizer.vocab_size, len(tokenizer.rare_words)\n",
        "    ),\n",
        "    tokenizer.vocab_list[:25]\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R29r-mAIpML2",
        "colab_type": "text"
      },
      "source": [
        "This still very basic tokenizer has _dramatically_ reduced our vocabulary size, from 14k/35k non-rare/rare words to 4k/15k, respectively (and, note that here we're being much more aggressive with the words we keep, aggregating words that appear less than 15 times).\n",
        "\n",
        "However, tokenization is still only the first step. Now, we need to build our real training corpus.\n",
        "\n",
        "### An _n_-gram, Auxiliary Features Baseline\n",
        "Our new model will be a logistic regression model trained using trigram information, concatenating the target word with the prior word and the subsequent word, along with all three word's auxiliary features (capitalization status and use of colons)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E7Uub8f5r-oI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def to_onehot_ngram(\n",
        "    indexed_dataset, n_pre=1, n_post=1, vocab_idxmap=tokenizer.vocab_idxmap,\n",
        "    labels_idxmap=tokenizer.labels_idxmap\n",
        "):\n",
        "  out_words, out_features, out_labels, sent_lengths = [], [], [], []\n",
        "  pad_and_feats = (\n",
        "      vocab_idxmap[PAD], tuple([0] * len(indexed_dataset[0][0][0][1]))\n",
        "  )\n",
        "  vocab_size = len(vocab_idxmap)\n",
        "  for sent in indexed_dataset:\n",
        "    N = len(sent)\n",
        "    sent_lengths.append(N)\n",
        "    for i in range(N):\n",
        "      (word, feats), label = sent[i]\n",
        "      \n",
        "      context_words = []\n",
        "      context_feats = []\n",
        "      for idx in range(i - n_pre, i+1 + n_post):\n",
        "        if idx == i: continue\n",
        "        c_word, c_feats = pad_and_feats if idx < 0 or idx >= N else sent[idx][0]\n",
        "        context_words.append(c_word)\n",
        "        context_feats.extend(list(c_feats))\n",
        "      out_words.append([word] + context_words)\n",
        "      out_features.append(list(feats) + context_feats)\n",
        "      out_labels.append(label)\n",
        "\n",
        "  categories = np.array([range(vocab_size)] * (1 + n_pre + n_post))\n",
        "  enc = OneHotEncoder(categories=categories)\n",
        "  out_dataset = enc.fit_transform(out_words)\n",
        "  assert (enc.categories_ == categories).all().all(), \"This can't change...\"\n",
        "\n",
        "  out_dataset = hstack((out_dataset, csr_matrix(out_features)), format='csr')\n",
        "\n",
        "  out_labels = [\n",
        "    labels_idxmap[l if l in labels_idxmap else PAD] for l in out_labels\n",
        "  ]\n",
        "\n",
        "  return out_dataset, out_labels"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zKEqAYt9wUgp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_sparse_X, train_Y = to_onehot_ngram(train_tokenized_index, n_pre=2)\n",
        "dev_sparse_X, dev_Y = to_onehot_ngram(tokenizer.transform(dev), n_pre=2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o1j0o-fIxyIf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\"Generated train dataset of type %s, shape %s, with %d labels\" % (\n",
        "  type(train_sparse_X), str(train_sparse_X.shape), len(train_Y)\n",
        "))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "abHzyir10pvf",
        "colab_type": "text"
      },
      "source": [
        "In the interest of time, we'll run a minimized model first, to see what performance it obtains quickly."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qPQA6E0vxxog",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%time\n",
        "short_model = LogisticRegression(\n",
        "    penalty='l2',\n",
        "    C=0.1,\n",
        "    max_iter=25,\n",
        ")\n",
        "short_model.classes_ = list(range(tokenizer.num_classes))\n",
        "\n",
        "short_model.fit(train_sparse_X, train_Y)\n",
        "\n",
        "acc = short_model.score(dev_sparse_X, dev_Y)\n",
        "print(\"Model accuracy (*not* Macro F1): %.2f%%\" % (100 * acc))\n",
        "\n",
        "dev_probs = short_model.predict_proba(dev_sparse_X)\n",
        "new_F1 = f1_score(dev_probs.argmax(axis=1), dev_Y, average='macro')\n",
        "print(\"The improved baseline Macro F1: %.2f\" % new_F1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PQoPc-4A0udJ",
        "colab_type": "text"
      },
      "source": [
        "Final results could definitely still be improved by running for more iterations, but at the cost of more time. Running for 200 iterations, for example, takes roughly 5 minutes for a gain of only 0.09 F1 points:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oES7gIxH8z5c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%time\n",
        "model = LogisticRegression(\n",
        "    penalty='l2',\n",
        "    C=0.1,\n",
        "    max_iter=200,\n",
        ")\n",
        "model.classes_ = list(range(tokenizer.num_classes))\n",
        "\n",
        "model.fit(train_sparse_X, train_Y)\n",
        "\n",
        "acc = model.score(dev_sparse_X, dev_Y)\n",
        "print(\"Model accuracy (*not* Macro F1): %.2f%%\" % (100 * acc))\n",
        "\n",
        "dev_probs = model.predict_proba(dev_sparse_X)\n",
        "new_F1 = f1_score(dev_probs.argmax(axis=1), dev_Y, average='macro')\n",
        "print(\"The improved baseline Macro F1: %.2f\" % new_F1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8yuigGKFV9DG",
        "colab_type": "text"
      },
      "source": [
        "### Questions on Data & Baselines\n",
        "  1. Based on your understanding of the task (and by exploring some of the real data), Assign labels to the following sequence of words:\n",
        "    > \"The patient (MRN: 1234) is a 86 y/o M, presenting at Hospital for Sick Patients on 12-10-2063\"\n",
        "\n",
        "  Assume that \"Hospital for Sick Patients\" is the official name of the institution providing care. Note that \"MRN\" stands for \"Medical Record Number\" and \"y/o\" stands for \"year old\" (these are both common medical abbreviations).\n",
        "  \n",
        "  2. In our presented code, we provide a very simple, unigram baseline and a more complex, trigram baseline and evaluate both on dev data. Evaluate both on test data, stored in `2014_test.tsv` and report the macro F1s obtained.\n",
        "\n",
        "The remaining questions will center around design choices made in our improved baseline, specifically focused on these parameters:\n",
        "  - `rare_words_thresh`\n",
        "  - `auxiliary_features`\n",
        "  - `lowercase`\n",
        "  - `remove_punct`\n",
        "  - `replace_only_punct`\n",
        "  - `replace_numbers`\n",
        "  - `n_pre`\n",
        "  - `n_post`\n",
        "  - `C`\n",
        "  - `penalty`\n",
        "\n",
        "  3. Between our provided tokenizer class, the one-hot converter function, and the Logistic Regression model itself, there are lots of changes we've made from the simpler unigram baseline, most represented through hyperparameters. Modifying all (or a subset) of the parameters listed above, find a parameter setting that matches the performance of the original unigram model on the dev set. What parameters acheive most similar performance to the original unigram model? Your answer should be supported by experimental results. If your parameter choices are correct, it is acceptable if the final performance you find is actually _worse_ than what we find with our simpler baseline above.\n",
        "\n",
        "  __Hint__: Consider under what settings will the tokenizer output data that looks most like the raw, unprocessed data?\n",
        "\n",
        "  __Hint 2__: Use a small Logistic regression model (e.g., `max_iter=25`) for iteration speed here. It's acceptable if your final experimental results are using this configuration.\n",
        "\n",
        "  4. The generic setting used in the cells above already acheive a significant performance improvement over the unigram baseline. What parameters of the list above are most influential in driving that performance gain?\n",
        "\n",
        "  5. Experiment with various other combinations of these parameters. In order for things to run efficiently, ensure that `n_pre + n_post` is no more than 4. What parameters seem to matter more generally? Your answer should be supported by dev-set evaluation results. Does it make sense to you that these parameters matter more than others? What is the best performance you can get with any parameters you try? TAs were able to get performance up to 0.7 -- can  you match that?\n",
        "\n",
        "  6. (Optional, not for credit) What's the _best *best*_ final performance you can obtain with a Baseline system here? Here, to be fair, you should first find your optimal configuration via dev-set experiments, and only evaluate on the test set immediately prior to readying your final writeup. Can you beat the TAs solution? "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NieBpn5Dmjmr",
        "colab_type": "text"
      },
      "source": [
        "## Using a sequential model\n",
        "For this section, we'll refine our approach to the above task by using a sequential, natural language processing model. This task _does_ rely on the same datasets used in the previous part, and some of the code used earlier as well, so if you've restarted your environment for some reason, make sure you re-run those cells. Additionally, unlike in the prior section, here you will need to fill in some missing code blocks (preceeded by large, obvious comments in the code) in order for things to run and work properly. So, the code below *__will not__* run as it stands, but should run seamlessly once you've fixed it according to the provided instructions.\n",
        "\n",
        "Note that this section extensively uses PyTorch, one of the most popular current deep learning libraries. We will _not_ explain introductory PyTorch concepts here; instead, we expect you to be roughly familiar with the basics of deep learning software, and to supplement your background with external resources as needed to be comfortable with basic PyTorch. Some good options for external resources are the PyTorch Tutorials, available [here](https://pytorch.org/tutorials/). We will, however, explain what we're doing, and offer a brief outline of the classes we use, design decisions we make, and common patterns or antipatterns in PyTorch design.\n",
        "\n",
        "Similarly, we also do not explain in depth the theory behind the model we ultimately use here, a Gated Recurrent Unit (GRU) recurrent neural network (RNN) model. If you'd like to learn more about GRUs, they are explained in more depth in [this](https://towardsdatascience.com/illustrated-guide-to-lstms-and-gru-s-a-step-by-step-explanation-44e9eb85bf21) blogpost, as well as (subordinately to LSTMs, but still) in [this](https://colah.github.io/posts/2015-08-Understanding-LSTMs/) excellent post by Cristopher Olah.\n",
        "\n",
        "Now, onto some pytorch boilerplate.\n",
        "### PyTorch Datasets\n",
        "PyTorch's `torch.utils.data.Dataset` class and corresponding `torch.utils.data.dataloader` class are the primary ways to encapsulate your data for use in a PyTorch neural network model. The `Dataset` class is a base class, with two mandatory abstract methods: `__len__` and `__getitem__`. As its name indicates, `__len__` must return the _length_ of the dataset stored in your subclass---i.e., the number of samples (_not_ the number of batches or anything like this, the raw number of samples in total). Similarly well-named, `__getitem__` takes an integral index and must return pytorch tensors corresponding to the sample at that index in the data. Note that there _are_ constraints on the shapes of the tensors returned by `__getitem__`; namely, they must match for all indices passed to `__getitem__` (unless you define a specialized collate function, which we do not here). This is because the PyTorch dataloader utilities will call `__getitem__` for each element of a batch and collate the results into batched tensors. In general, this fact also implies it is wise to push as much of the processing in your `Dataset` subclass outside of the `__getitem__` path as possible so it isn't run each sample of your batch, and is instead run once (e.g., at construction) then stored for repeated use.\n",
        "\n",
        "In this assignment, we've nearly completely defined the dataset for you; however, you *__will__* have to modify things, as we task you with completing some broken code blocks. Accordingly, this code *__will not__* run as it is now--you'll need to fix it first."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G2_kVC67vVhS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch, torch.nn as nn, torch.utils.data as data, torch.optim as optim\n",
        "from allennlp.modules.time_distributed import TimeDistributed\n",
        "\n",
        "class NERDataset(data.Dataset):\n",
        "  def __init__(\n",
        "      self, dataset_list, tokenizer, max_len=None, already_indexed=False,\n",
        "      get_char_sequence=True, keep_chars_for_rare_words=True,\n",
        "      use_cuda=torch.cuda.is_available()\n",
        "  ):\n",
        "    \"\"\"\n",
        "    dataset_list: [ sentence 1: [((word_1, (auxiliary_features...)), label_1), ...], ...]\n",
        "    \"\"\"\n",
        "    super().__init__()\n",
        "\n",
        "    self.dataset_list = dataset_list\n",
        "    self.num_aux_features = len(tokenizer.auxiliary_features)\n",
        "    self.N = len(dataset_list)\n",
        "    self.__set_index(max_len)\n",
        "\n",
        "    self.vocab_idxmap = tokenizer.vocab_idxmap\n",
        "    assert UNK in self.vocab_idxmap and PAD in self.vocab_idxmap\n",
        "    self.labels_idxmap = tokenizer.labels_idxmap\n",
        "    assert PAD in self.labels_idxmap\n",
        "    self.use_cuda = use_cuda\n",
        "\n",
        "    assert not already_indexed and get_char_sequence, \"Can't do both.\"\n",
        "\n",
        "    self.tokenizer = tokenizer\n",
        "    self.get_char_sequence = get_char_sequence\n",
        "    self.keep_chars_for_rare_words = True\n",
        "    if self.get_char_sequence:\n",
        "      self.max_word_len = max(\n",
        "          len(w) for s in self.dataset_list for (w, _), _ in s\n",
        "      )\n",
        "        \n",
        "    self.already_indexed = already_indexed\n",
        "\n",
        "  def __set_index(self, global_max_len):\n",
        "    self.max_len = max(len(sent) for sent in self.dataset_list)\n",
        "    if global_max_len is None or self.max_len < global_max_len:\n",
        "      self.index = [(i, 0) for i in range(self.N)]\n",
        "      return\n",
        "\n",
        "    self.max_len = global_max_len\n",
        "    self.index = []\n",
        "    for i, sent in enumerate(self.dataset_list):\n",
        "      if len(sent) < self.max_len:\n",
        "        self.index.append((i, 0))\n",
        "        continue\n",
        "      \n",
        "      for idx in range(0, len(sent), self.max_len):\n",
        "        self.index.append((i, idx))      \n",
        "\n",
        "  def __len__(self): return len(self.index)\n",
        "\n",
        "  def __to_tensor(self, arr):\n",
        "    tens = torch.Tensor(arr).long()\n",
        "    if self.use_cuda: tens = tens.cuda()\n",
        "    return tens\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    sent_idx, start_idx = self.index[idx]\n",
        "    sent = self.dataset_list[sent_idx][start_idx:start_idx+self.max_len]\n",
        "\n",
        "    X, C, C_mask, A, Y = [], [], [], [], []\n",
        "    # A simple modification here enables this to run on raw data.\n",
        "    for (w_orig, aux), l in sent:\n",
        "      w = w_orig\n",
        "\n",
        "      ## TODO: Task #1: ########################################################\n",
        "      ## For this task, you need to fill in the Dataset construction logic    ##\n",
        "      ## for this part of `__getitem__`. In each clause of the below if       ##\n",
        "      ## statement, some lines will be left incomplete. Fill them in,         ##\n",
        "      ## remembering what the point of this method is: Given an input         ##\n",
        "      ## sequence of words, return:                                           ##\n",
        "      ##   1) A tensor `mask` of shape [self.max_len,] containing a mask with ##\n",
        "      ##      1s indicating whether or not there was a real word present at   ##\n",
        "      ##      that position. This tensor is created later in the function,and ##\n",
        "      ##      you needn't worry about it here.                                ##\n",
        "      ##   2) A tensor `X` of shape [self.max_len,] containing an integral    ##\n",
        "      ##      index (based on the map from word -> index in                   ##\n",
        "      ##      `self.vocab_idxmap`) for each word in the sentence. You will    ##\n",
        "      ##      help create `X` here.                                           ##\n",
        "      ##   3) (If `self.get_char_sequence`): A tensor `C` of shape            ##\n",
        "      ##      [self.max_len, self.max_word_len] containing a sequence of      ##\n",
        "      ##      sequence of integral indexs corresponding to each letter in     ##\n",
        "      ##      each word (via the mapping in `self.tokenizer.chars_idxmap`).   ##\n",
        "      ##   4) (If `self.get_char_sequence`): A tensor `C_mask` of shape       ##\n",
        "      ##      [self.max_len, self.max_word_len] containing a sequence of      ##\n",
        "      ##      sequence of masks indicating which characters were present in   ##\n",
        "      ##      which words.                                                    ##\n",
        "      ##   5) A tensor `A` of shape [self.max_len, self.num_aux_features]     ##\n",
        "      ##      containing the auxiliary features per token. You won't need to  ##\n",
        "      ##      do anything with `A` here.                                      ##\n",
        "      ##   6) A tensor `Y` of shape [self.max_len,] containing the labels per ##\n",
        "      ##      token. You won't need to adjust `Y` here.                       ##\n",
        "      ## In this section, you'll need to help us construct some of these      ##\n",
        "      ## pieces.                                                              ##\n",
        "      ##########################################################################\n",
        "\n",
        "      if self.already_indexed: \n",
        "        # In this case, we don't need to change anything--the indexing has\n",
        "        # already been done.\n",
        "        X.append(w)\n",
        "      else: \n",
        "        # TODO: Fill this in. Here, we need to convert `w` to an index, using\n",
        "        # the map `self.vocab_idxmap`. Note that `w` may not be in this map, in\n",
        "        # which case we want to convert it to an UNK token before indexifying\n",
        "        # it. Once you've converted it, append it to `X`. Feel free to use\n",
        "        # helper variables.\n",
        "\n",
        "        X.append(...)\n",
        "      if self.get_char_sequence:\n",
        "        if self.keep_chars_for_rare_words: w_chars = w_orig\n",
        "        else: w_chars = w\n",
        "\n",
        "        if w_chars in (UNK, PAD, PUNCT): w_chars = ''\n",
        "        delta = self.max_word_len - len(w_chars)\n",
        "\n",
        "        # TODO: Fill this in. Here, we have `w_chars` and delta -- now we just\n",
        "        # need to convert the characters in w_chars to indices and pad the list\n",
        "        # to self.max_word_len.\n",
        "\n",
        "        cseq = ...\n",
        "        cseq += ...\n",
        "        C.append(cseq)\n",
        "\n",
        "        ## End Task #1 #########################################################\n",
        "\n",
        "        C_mask.append([1] * len(w_chars) + [0] * delta)\n",
        "\n",
        "      A.append(list(aux))\n",
        "\n",
        "      l = l if l in self.labels_idxmap else PAD #We ignore labels we don't know.\n",
        "      Y.append(self.labels_idxmap[l])\n",
        "    \n",
        "    delta = self.max_len - len(X)\n",
        "    if delta > 0:\n",
        "      mask = [1] * len(X) + [0] * delta\n",
        "      X.extend([self.vocab_idxmap[PAD]] * delta)\n",
        "      if self.get_char_sequence:\n",
        "        # Here, by construction, we rely on max_word_len being always larger\n",
        "        C.extend([[self.tokenizer.chars_idxmap[PAD]]*self.max_word_len] * delta)\n",
        "        C_mask.extend([[0]*self.max_word_len] * delta)\n",
        "      A.extend([[0] * self.num_aux_features] * delta)\n",
        "      Y.extend([self.labels_idxmap[PAD]] * delta)\n",
        "    else:\n",
        "      mask = [1] * self.max_len\n",
        "      X = X[:self.max_len]\n",
        "      A = A[:self.max_len]\n",
        "      if self.get_char_sequence:\n",
        "        C = C[:self.max_len]\n",
        "        C_mask = C_mask[:self.max_len]\n",
        "      Y = Y[:self.max_len]\n",
        "\n",
        "    if self.get_char_sequence:\n",
        "      return (\n",
        "          self.__to_tensor(mask), self.__to_tensor(X), self.__to_tensor(C),\n",
        "          self.__to_tensor(C_mask), self.__to_tensor(A).float(),\n",
        "          self.__to_tensor(Y)\n",
        "      )\n",
        "    else:\n",
        "      return (\n",
        "          self.__to_tensor(mask), self.__to_tensor(X),\n",
        "          self.__to_tensor(A).float(), self.__to_tensor(Y)\n",
        "      )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w0kb51Oci2XN",
        "colab_type": "text"
      },
      "source": [
        "To see what the dataset produces, we can instantiate one here and call it, using our tokenizer class from 2.1 here as well. Calling `train_dataset[0]` at the end (equivalent to `train_dataset.__getitem__(0)`) returns the tensors corresponding to the first example in our train dataset. Note these tensors are all padded out to the passed `max_len` (50), which is necessary here so all entries collate properly. The entries in the padding positions will be ignored in both model running and loss calculation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KVCWfhkwxkvY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nn_tokenizer = Tokenizer(rare_words_thresh=10)\n",
        "nn_tokenizer.fit(train)\n",
        "train_tokenized = nn_tokenizer.transform(train, as_index=False)\n",
        "\n",
        "train_dataset = NERDataset(\n",
        "    train_tokenized, nn_tokenizer, max_len=50, already_indexed=False\n",
        ")\n",
        "\n",
        "print(train_dataset.max_word_len)\n",
        "for i in train_dataset[0]:\n",
        "  print(i.shape)\n",
        "  display(i[:2])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "albci3Cjjbjq",
        "colab_type": "text"
      },
      "source": [
        "### Model Class & Training Loop\n",
        "\n",
        "Now onto the model class and training loop logic for this problem. Note that here, you *__will__* have to modify things, as we task you with completing some broken code blocks. Accordingly, this code *__will not__* run as it is now--you'll need to fix it first. We'll give a brief overview of each section here, and have comments within the code. \n",
        "\n",
        "#### Model Class\n",
        "Pytorch model classes, also called _modules_, are all subclassed from `torch.nn.Module`, which contains much of the logic necessary for, among other things, automatic differentiation. There are different schools of thought on the best way to design modules, from an engineering perspective. One approach is to try to make many small modules, each of which is relatively isolated and encompasses only a small part of the model, then assemble them heirarchically into a larger whole. This approach is more modular and flexible, which can be very advantageous in research. Another is to have most of your model in a few larger modules, which will often go so far as to compute final losses or normalized probabilities straight from raw input. This approach argues that, with research use cases often requiring nuanced, individualized support, trying to re-use smaller modules risks making code more brittle and dependent on more special-cased support, and that it is better to stick with only the basic primitives provided by torch directly.\n",
        "\n",
        "Here, given our use case is narrow and limited, we've defined just one `nn.Module` subclass, called `NER_GRU`. This class, on `forward`, ingests `mask`, a tensor describing which words are actually present in this sentence, the tokenized word sequence `X`, a tensor `aux` containing the auxiliary features output by the tokenizer (e.g., capitalization status, etc.), and the final output labels (numerically encoded) in `labels`. It first embeds the words via a trainable, randomly initialized word embedding layer, adds to these embeddings the auxiliary features, projected into the embedding layer via a linear transformation, runs this sequence through a GRU parametrized by the hyperparameters passed, passes each sequential output of the GRU through a linear output layer, then scores the whole system with a `CrossEntropyLoss` against the provided labels. It also normalizes the per-token scores into probabilities via a `nn.Softmax` layer and passes these probabilities out as well as the loss."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HYhqlkT82uoS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SeqModelTypes(Enum):\n",
        "  NONE = auto()\n",
        "  GRU = auto()\n",
        "  LSTM = auto()\n",
        "\n",
        "class NER_Model(nn.Module):\n",
        "  SeqModelConstructors = {\n",
        "      SeqModelTypes.GRU: nn.GRU,\n",
        "      SeqModelTypes.LSTM: nn.LSTM,\n",
        "  }\n",
        "\n",
        "  def __init__(\n",
        "      self,\n",
        "      tokenizer = nn_tokenizer,\n",
        "      embed_size = 50,\n",
        "      hidden_size = 150,\n",
        "      num_layers = 1,\n",
        "      dropout = 0.1,\n",
        "      bidirectional = False,\n",
        "      rnn_type=SeqModelTypes.LSTM,\n",
        "      char_rnn_type=SeqModelTypes.NONE, # if NONE, params below ignored.\n",
        "      char_embed_size=25,\n",
        "      char_rnn_hidden_size=50,\n",
        "      char_rnn_num_layers=1,\n",
        "      char_rnn_bidirectional=True,\n",
        "  ):\n",
        "    assert rnn_type != SeqModelTypes.NONE\n",
        "\n",
        "    super().__init__()\n",
        "\n",
        "    # We use the tokenizer as a convenient storage location for the vocab and\n",
        "    # label indexmapstest = read_dataset_file('2014_test.tsv'), as well as the number of auxiliary features.\n",
        "    self.vocab_idxmap = tokenizer.vocab_idxmap\n",
        "    self.labels_idxmap = tokenizer.labels_idxmap\n",
        "    self.num_aux_features = len(tokenizer.auxiliary_features)\n",
        "    self.char_idxmap = tokenizer.chars_idxmap\n",
        "\n",
        "    # Embedding hyperparameters\n",
        "    self.vocab_size = len(self.vocab_idxmap)\n",
        "    self.char_vocab_size = len(self.char_idxmap)\n",
        "    self.embed_size = embed_size\n",
        "    self.char_embed_size = char_embed_size\n",
        "\n",
        "    # GRU Hyperparameters\n",
        "    self.hidden_size = hidden_size\n",
        "    self.num_layers = num_layers\n",
        "    self.dropout = dropout\n",
        "    self.bidirectional = bidirectional\n",
        "    self.rnn_type = rnn_type\n",
        "    self.char_rnn_type = char_rnn_type\n",
        "    self.embed_char_seq = (self.char_rnn_type != SeqModelTypes.NONE)\n",
        "    self.char_rnn_hidden_size = char_rnn_hidden_size\n",
        "    self.char_rnn_num_layers = char_rnn_num_layers\n",
        "    self.char_rnn_bidirectional = char_rnn_bidirectional\n",
        "\n",
        "    # Output Hyperparameters\n",
        "    self.num_classes = len(self.labels_idxmap) - 1 # -1 for padding\n",
        "\n",
        "    # Model parts\n",
        "    self.embed_layer = nn.Embedding(\n",
        "        self.vocab_size, self.embed_size, padding_idx=self.vocab_idxmap[PAD],\n",
        "    )\n",
        "    if self.num_aux_features:\n",
        "      # We only need to produce this layer if we have any aux features...\n",
        "      self.aux_embed_layer = nn.Linear(self.num_aux_features, self.embed_size)\n",
        "    \n",
        "    sent_rnn_params = dict(\n",
        "      input_size    = self.embed_size,\n",
        "      hidden_size   = self.hidden_size,\n",
        "      num_layers    = self.num_layers,\n",
        "      batch_first   = True,\n",
        "      dropout       = self.dropout,\n",
        "      bidirectional = self.bidirectional,\n",
        "    )\n",
        "    self.rnn = NER_Model.SeqModelConstructors[rnn_type](**sent_rnn_params)\n",
        "\n",
        "    if self.embed_char_seq:\n",
        "      self.char_embed_layer = nn.Embedding(\n",
        "          self.char_vocab_size, self.char_embed_size,\n",
        "          padding_idx=self.char_idxmap[PAD]\n",
        "      )\n",
        "      char_rnn_params = dict(\n",
        "        input_size    = self.char_embed_size,\n",
        "        hidden_size   = self.char_rnn_hidden_size,\n",
        "        num_layers    = self.char_rnn_num_layers,\n",
        "        batch_first   = True,\n",
        "        dropout       = self.dropout,\n",
        "        bidirectional = self.char_rnn_bidirectional,\n",
        "      )\n",
        "\n",
        "      # Time distributed expects a simple module, which returns a tensor.\n",
        "      # Unfortunately, pytorch RNNs don't do this. So we use a virtual module\n",
        "      # to account for this.\n",
        "      class SimpleOutputRNN(nn.Module):\n",
        "        def __init__(self, rnn_cnstr, rnn_kwargs):\n",
        "          super().__init__()\n",
        "\n",
        "          self.rnn = rnn_cnstr(**rnn_kwargs)\n",
        "          if 'num_layers' in rnn_kwargs:\n",
        "            self.num_layers = rnn_kwargs['num_layers']\n",
        "          else: self.num_layers = 1\n",
        "          if 'bidirectional' in rnn_kwargs and rnn_kwargs['bidirectional']:\n",
        "            self.num_directions = 2\n",
        "          else: self.num_directions = 1\n",
        "\n",
        "          self.hidden_size = rnn_kwargs['hidden_size']\n",
        "\n",
        "        def forward(self, seq):\n",
        "          \"\"\"\n",
        "            `seq` is of shape (augmented batch_size, rnn_seq_len, hidden_dim)\n",
        "          \"\"\"\n",
        "          aug_batch_size, seq_len, _ = seq.shape\n",
        "          rnn_out = self.rnn(seq)[0]\n",
        "          # if type(rnn_out) is tuple: rnn_out = rnn_out[0]\n",
        "          return rnn_out.view(aug_batch_size, seq_len, -1)\n",
        " \n",
        "      self.char_rnn = TimeDistributed(SimpleOutputRNN(\n",
        "          NER_Model.SeqModelConstructors[self.char_rnn_type], char_rnn_params\n",
        "      ))\n",
        "\n",
        "      self.char_num_directions = (2 if self.char_rnn_bidirectional else 1)\n",
        "      char_rnn_out_dim = self.char_num_directions * self.char_rnn_hidden_size\n",
        "      self.char_seq_embed_layer = nn.Linear(char_rnn_out_dim, self.embed_size)\n",
        "\n",
        "    num_directions = (2 if self.bidirectional else 1)\n",
        "    rnn_out_dim = num_directions * self.hidden_size\n",
        "\n",
        "    self.output_layer = nn.Linear(rnn_out_dim, self.num_classes)\n",
        "\n",
        "    # Loss & Output normalization\n",
        "    self.softmax = nn.Softmax(dim=2)\n",
        "    self.loss_fn = nn.CrossEntropyLoss(ignore_index = self.labels_idxmap[PAD])\n",
        "\n",
        "  def forward(self, *inputs):\n",
        "    \"\"\"\n",
        "    Inputs:\n",
        "      `mask` is of shape (batch_size, seq_len), contains 1 if word is present, 0\n",
        "        otherwise.\n",
        "      `X` is of shape (batch_size, seq_len), contains ids of words to be\n",
        "        embedded.\n",
        "      `C` is of shape (batch_size, seq_len, word_len), contains ids of chars\n",
        "        within words to be embedded. Only present if self.embed_char_seq.\n",
        "      `C_mask` is of shape (batch_size, seq_len, word_len), contains 1 if char\n",
        "        is present, 0 otherwise. Only present if self.embed_char_seq.\n",
        "      `aux` is of shape (batch_size, seq_len, self.num_aux_features), contains any\n",
        "        auxiliary features computed by the tokenizer (e.g., is_capitalized)\n",
        "      `labels` is of shape (batch_size, seq_len), contains ids of labels per\n",
        "        token.\n",
        "    Outputs:\n",
        "      `probs` is of shape (batch_size, seq_len, self.num_classes). Contains\n",
        "        normalized probabilities per token, per class. All probabilities for\n",
        "        padding tokens have been zeroed out.\n",
        "      `loss` is a scalar, containing the mean cross entropy loss across all\n",
        "        tokens and all sentences. Note that this loss doesn't do anything clever\n",
        "        w.r.t. normalizing for sentence length --- i.e., it is not a \n",
        "        averaged per-sentence/per-example loss, but rather a per-token loss. For\n",
        "        our purposes this is fine.\n",
        "    \"\"\"\n",
        "\n",
        "    if self.embed_char_seq:\n",
        "      mask, X, C, C_mask, aux, labels = inputs\n",
        "    else:\n",
        "      mask, X, aux, labels = inputs\n",
        "\n",
        "    # For this problem, we ask you to fill in part of the forward method of this\n",
        "    # neural network. Recall our stated plan.\n",
        "    #  1. First, we're going to embed the input `X` (recall we've initialized\n",
        "    #     an embedding layer already in `self.embed_layer`.\n",
        "    #  2. Next, if there are any auxiliary features, we'll project those into\n",
        "    #     into the embedding space via our projection layer\n",
        "    #     `self.aux_embed_layer`, and add them to the embedded words.\n",
        "    #  3. Embed the character sequence. This will be a miniature version of our\n",
        "    #     whole pipeline. First, we embed them according to the embedding layer\n",
        "    #     `self.char_embed_layer`, then we pass them through our (modified)\n",
        "    #     `self.char_rnn`, followed by the char rnn output layer \n",
        "    #     `self.char_seq_embed_layer`. Finally, we add them to our embedded\n",
        "    #     words \n",
        "    #  4. We pass this featurized embedding tensor through `self.rnn`, keeping\n",
        "    #     the output at each token via the first element of the pytorch RNN\n",
        "    #     output tuple.\n",
        "    #  5. We send these per-token output through `self.output_layer` to get our \n",
        "    #     scores.\n",
        "    #  6. Finally, we compute our loss and probabilities, and return them.\n",
        "\n",
        "    # First we embed, always the words and the auxiliary features as well if\n",
        "    # present.\n",
        "\n",
        "    token_features = []\n",
        "    token_features.append(self.embed_layer(X))\n",
        "\n",
        "    if self.num_aux_features: \n",
        "      token_features.append(self.aux_embed_layer(aux.float()))\n",
        "\n",
        "    if self.embed_char_seq:\n",
        "      # Now we embed the character sequence\n",
        "      embedded_chars = self.char_embed_layer(C)\n",
        "      char_rnn_out = self.char_rnn(embedded_chars)\n",
        "      # `char_rnn_out` is of shape (batch, seq, word_len, hidden_dim)\n",
        "      # We take an average pooling over the word-level, subject to C_mask.\n",
        "\n",
        "      # `C_mask` is of shape (batch, seq, word_len)\n",
        "      char_rnn_mask = C_mask.unsqueeze(3).expand_as(char_rnn_out)\n",
        "      char_rnn_out = (char_rnn_out * char_rnn_mask).sum(dim=2)\n",
        "      word_lens = C_mask.sum(dim=2).unsqueeze(2).expand_as(char_rnn_out)\n",
        "      \n",
        "      # Here, we're really just computing char_rnn_out / word_lens. Except,\n",
        "      # for padding words, word_len can be 0, so we need to catch that case,\n",
        "      # and zero it out, then divide by something that looks like word_lens\n",
        "      # otherwise or is 1 to avoid illusory nans. We use torch.where to catch\n",
        "      # these conditionals.\n",
        "      word_lens_or_one = torch.where(\n",
        "          word_lens == 0, torch.ones_like(word_lens), word_lens\n",
        "      )\n",
        "      char_rnn_out = torch.where(\n",
        "          word_lens == 0, torch.zeros_like(char_rnn_out),\n",
        "          char_rnn_out / word_lens_or_one\n",
        "      )\n",
        "\n",
        "      char_rnn_out = self.char_seq_embed_layer(char_rnn_out)\n",
        "\n",
        "      token_features.append(char_rnn_out)\n",
        "\n",
        "    #### TODO: Task #2: ########################################################\n",
        "    ##   Combine all the per-token features together, then pass them through  ##\n",
        "    ##   `self.rnn` and `self.output_layer` to get our per-token, per-class   ##\n",
        "    ##   scores.                                                              ##\n",
        "    ##   *Hint:* Remember that PyTorch's RNN methods return tuples, and you   ##\n",
        "    ##   only need part of the tuple here.                                    ##\n",
        "    ############################################################################\n",
        "    embedded_token_seq = # TODO: Fill this part in!\n",
        "    rnn_out = # TODO: Fill this part in!\n",
        "    scores = # TODO: Fill this part in!\n",
        "    #### End Task #2 ###########################################################\n",
        "\n",
        "\n",
        "    # scores is of shape (batch_size, seq_len, num_classes), but the loss\n",
        "    # expects data of the shape (batch_size, num_classes, d_1, d_2, ...) and\n",
        "    # labels of the shape (batch_size, d_1, d_2, ...), so we need to reshape:\n",
        "    scores_reshaped = scores.transpose(1, 2)\n",
        "    labels_reshaped = labels # This is actually fine as is\n",
        "    loss = self.loss_fn(scores_reshaped, labels_reshaped)\n",
        "\n",
        "    # We comptue probabilities here for convenience later.\n",
        "    probs = self.softmax(scores) * mask.unsqueeze(2).expand_as(scores)\n",
        "    \n",
        "    return probs, loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E638a6_qo4gJ",
        "colab_type": "text"
      },
      "source": [
        "#### Train & Evaluation Loops\n",
        "We also need to write our training and evaluation loops here. Note that there are libraries to automate some of this logic, most notably PyTorch's own [Ignite](https://pytorch.org/ignite/), which we do not use here for simplicity but are very useful for larger projects.\n",
        "\n",
        "Our train and evaluation loops are simple. In the `eval_model` function, we simply create a deterministic (i.e., `shuffle=False`) `DataLoader` over the dev set, then iterate through this dataloader and collate the final probabilities. These are then re-structured to match the original data formatting and passed into the `ner_eval` function from part 1. We default to large `batch_size` in this function because larger batches can afford greater efficiencies in terms of GPU usage, and the validation batch size will not affect performance as no gradients are computed during evaluation.\n",
        "\n",
        "In the `run_model` function, we use a very similar setup the `eval_model` function just described, save that we run over the train dataset many times, once per epoch, optimizing the parameters via the [`Adam` optimizer](https://pytorch.org/docs/stable/optim.html?highlight=adam#torch.optim.Adam) each batch. Each epoch, we also evaluate the model on the validation data, so that we can track final performance. Note the hyperparameters passed to this function:\n",
        "  * `num_epochs`\n",
        "  * `batch_size`\n",
        "  * `learning_rate_init`\n",
        "\n",
        "All of these can have significant impacts on learning speed and final performance."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tr8M0yLu-Vnk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def eval_model(val_dataset, model, batch_size = 1024, debug=False):\n",
        "  val_dataloader = data.DataLoader(\n",
        "      val_dataset, batch_size=batch_size, shuffle=False\n",
        "  )\n",
        "  num_val_batches = len(val_dataloader)\n",
        "\n",
        "  model.eval()\n",
        "  val_batches = tqdm(\n",
        "      val_dataloader, total=num_val_batches, leave=False, desc=\"Val Batch\"\n",
        "  )\n",
        "  val_probs, val_probs_flat = [], []\n",
        "  val_labels, val_labels_flat = [], []\n",
        "  val_loss = 0\n",
        "  for batch in val_batches:\n",
        "    with torch.no_grad():\n",
        "      probs, loss = model(*batch)\n",
        "      val_loss += loss.detach().cpu().numpy()\n",
        "      probs = list(probs.detach().cpu().numpy())\n",
        "      mask = list(batch[0].detach().cpu().numpy())\n",
        "      labels = list(batch[-1].detach().cpu().numpy())\n",
        "    probs_ragged, probs_flat = [], []\n",
        "    labels_ragged, labels_flat = [], []\n",
        "    for probs_sent, mask_sent, labels_sent in zip(probs, mask, labels):\n",
        "      probs_ragged.append(probs_sent[:sum(mask_sent)])\n",
        "      labels_ragged.append(labels_sent[:sum(mask_sent)])\n",
        "      probs_flat.extend(probs_sent[:sum(mask_sent)])\n",
        "      labels_flat.extend(labels_sent[:sum(mask_sent)])\n",
        "    val_probs.extend(probs_ragged)\n",
        "    val_labels.extend(labels_ragged)\n",
        "    val_probs_flat.extend(probs_flat)\n",
        "    val_labels_flat.extend(labels_flat)\n",
        "\n",
        "  if debug: \n",
        "      return val_probs, val_dataset.dataset_list, val_dataset.labels_idxmap\n",
        "\n",
        "  val_loss /= num_val_batches\n",
        "  val_f1 = f1_score(val_labels_flat, np.argmax(val_probs_flat, axis=1), average='macro')\n",
        "\n",
        "  return val_loss, val_f1\n",
        "\n",
        "def run_model(\n",
        "    train_dataset, val_dataset, model,\n",
        "    num_epochs = 20,\n",
        "    batch_size = 64,\n",
        "    learning_rate_init = 3e-4,\n",
        "    val_batch_size=512,\n",
        "    step_size = 10,\n",
        "    gamma = 0.1,\n",
        "):\n",
        "  train_dataloader = data.DataLoader(\n",
        "      train_dataset, batch_size=batch_size, shuffle=True\n",
        "  )\n",
        "  num_train_batches = len(train_dataloader)\n",
        "\n",
        "  optimizer = optim.AdamW(model.parameters(), lr=learning_rate_init)\n",
        "  sch = optim.lr_scheduler.StepLR(\n",
        "      optimizer, step_size, gamma=gamma, last_epoch=-1\n",
        "  )\n",
        "\n",
        "  epoch_progress_temp = \"Epoch: Train Loss %.2e, Dev F1 %.2f\"\n",
        "  batch_progress_temp = \"Train Batch: %.2e\"\n",
        "\n",
        "  epochs = tqdm(\n",
        "      range(num_epochs), desc=epoch_progress_temp % (np.NaN, np.NaN)\n",
        "  )\n",
        "  val_f1s = []\n",
        "  train_losses = []\n",
        "  val_losses = []\n",
        "  for epoch in epochs:\n",
        "    model.train()\n",
        "    \n",
        "    train_batches = tqdm(\n",
        "        train_dataloader, total=num_train_batches, leave=False,\n",
        "        desc=batch_progress_temp % (np.NaN),\n",
        "    )\n",
        "    losses_epoch = []\n",
        "    for batch in train_batches:\n",
        "      ## TODO: Task #3: ########################################################\n",
        "      ## Here, you need to compute the actual training loop for this train    ##\n",
        "      ## function. Note that this section of code is relatively model         ##\n",
        "      ## agnostic. All you need to do is zero the gradients stored in our     ##\n",
        "      ## optimizer, compute the loss of the model on the batch (recall our    ##\n",
        "      ## model outputs both the probabilities and the loss on each call to    ##\n",
        "      ## forward, run the `backward` step of PyTorch's automatic              ##\n",
        "      ## differentiation system on the loss tensor, then `step` with the      ##\n",
        "      ## optimizer to adjust the model parameters.                            ##\n",
        "      ## Go through some basic PyTorch tutorials if this is unfamiliar to     ##\n",
        "      ## you. A paritcularly good place to look could be here:                ##\n",
        "      ## https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html,  ##\n",
        "      ## in particular, Section 4.\n",
        "      ##########################################################################\n",
        "\n",
        "      # Complete the optimizer here. Store the loss in a tensor called `loss`\n",
        "      # or the rest of the function will not work.\n",
        "\n",
        "      ## End Task #3 ###########################################################\n",
        "\n",
        "      loss_value = loss.detach().cpu().numpy()\n",
        "      train_batches.set_description(batch_progress_temp % loss_value)\n",
        "      losses_epoch.append(loss_value)\n",
        "\n",
        "    val_loss, val_f1 = eval_model(val_dataset, model, val_batch_size)\n",
        "\n",
        "    val_losses.append(val_loss)\n",
        "    val_f1s.append(val_f1)\n",
        "    train_losses.append(losses_epoch)\n",
        "\n",
        "    epochs.set_description(\n",
        "        epoch_progress_temp % (np.mean(losses_epoch[-20:]), val_f1)\n",
        "    )\n",
        "    sch.step()\n",
        "\n",
        "  return model, train_losses, val_losses, val_f1s"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZC9s13UdqmBT",
        "colab_type": "text"
      },
      "source": [
        "Finally, with all that done, it's time to actually train our model. Note two things:\n",
        "  1. For training purposes, we set the maximum length of any sentence over which we can train to 50 tokens. But, at validation time, we leave it unset, allowing the system to span sentences much longer. Is this sensible? What impact might this have on our training?\n",
        "  2. This cell will take a fairly long time (e.g., 15 - 20 minutes) to train/run. It will print output after the datasets are constructed, then show a progress bar (if this does not appear to be working properly, ensure you are using the most recent version of chrome, and recall that you should've restarted the runtime once after running the very first import cell of this notebook) during training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rhp6eL4bJOMr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%time\n",
        "nn_tokenizer = Tokenizer(\n",
        "  rare_words_thresh=RARE_WORDS_THRESHOLD,\n",
        "  auxiliary_features=[],\n",
        "  lowercase=True,\n",
        "  remove_punct=False,\n",
        "  replace_only_punct=False,\n",
        "  replace_numbers=NUM,\n",
        ")\n",
        "nn_tokenizer.fit(train)\n",
        "train_tokenized = nn_tokenizer.transform(train, as_index=False)\n",
        "dev_tokenized = nn_tokenizer.transform(dev, as_index=False)\n",
        "\n",
        "train_dataset = NERDataset(\n",
        "    train_tokenized, nn_tokenizer, max_len=100, already_indexed=False\n",
        ")\n",
        "val_dataset = NERDataset(\n",
        "    dev_tokenized, nn_tokenizer, max_len=None, already_indexed=False\n",
        ")\n",
        "\n",
        "print(\n",
        "    \"Produced train and validation datasets. \"\n",
        "    \"Validation max_len is %d\" % val_dataset.max_len\n",
        ")\n",
        "\n",
        "model = NER_Model(\n",
        "  tokenizer=nn_tokenizer,\n",
        "  embed_size=300,\n",
        "  dropout=0.5,\n",
        "  hidden_size=450,\n",
        "  num_layers=1,\n",
        "  bidirectional=True,\n",
        "  rnn_type = SeqModelTypes.LSTM,\n",
        "  char_rnn_type = SeqModelTypes.LSTM,\n",
        ")\n",
        "\n",
        "model.cuda()\n",
        "\n",
        "model, train_losses, val_losses, val_f1s = run_model(\n",
        "    train_dataset, val_dataset, model, num_epochs=30, learning_rate_init=3e-4,\n",
        "    gamma = 0.5, val_batch_size=256\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VTG7ICybrbYx",
        "colab_type": "text"
      },
      "source": [
        "### Your Deliverables\n",
        "#### Fixing the Neural Network\n",
        "In the code above, there are three major blocks of incomplete code: one within the `NERDataset` class's `__getitem__` function, one within the `NER_Model` class's `forward` function, and one within the `run_model` function. All are denoted with large comment blocks describing what is missing. Fill in these incomplete code sections. When fixed, you should obtain a dev-set macro F1 of approximately 0.75 after 20 epochs. Find and fix these issues, then answer the following questions:\n",
        "  1. How did you fill in these code sections? Be specific, including code snippets and commentary on what your code is doing at each step?\n",
        "  2. What final performance did you obtain, first, on the dev-set, and second, on the test-set? As with the baseline system, you will need to write code yourself to measure the performance on the test dataset.\n",
        "  3. (Optional, not for credit): What is the _best_ performance you can obtain with a neural network model? What hyperparameters did you use / changes did you make to obtain this?\n",
        "\n",
        "#### Comparison to the Baseline System\n",
        "We saw in part 1 that a number of hyperparameters were very impactful on the performance of the baseline system. Here, we'll explore some of those parameters for this model as well.\n",
        "  1. How sensitive is the system to the tokenization parameters `lowercase`, `remove_punct`, and `replace_numbers`? Is it similarly sensitive as the baseline system? Does this make sense?\n",
        "  2. More generally, are you surprised at the performance of the neural network relative to the baseline system? What can the neural network here do that our baseline system can't, and why might this be important for this task? What advantages does our baseline have over the neural network, and why might they be helpful? List at least 2 main differences between the neural network and the baseline system that could contribute to any performance discrepancy, and for each comment on why you expect that could be a major contributing factor. For each, also give a sample, synthetic sentence that illustrates a case where you think these differences could be important.\n",
        "\n",
        "#### Extensions\n",
        "These models, while thoughtful, could definitely be extended in many ways. Think of one way each the baseline model and the neural network models could be extended. For each of these, describe the extension and indicate why you think it might improve performance as well as any reasons it might _not_ improve performance (as it is always good to consider why something might not work as well as why it might).\n",
        "\n",
        "__Hint__: Check out the [paper](https://arxiv.org/abs/1606.03475) by Dernoncourt et al., for some ideas. There is a major component missing from both our models that is described in their system."
      ]
    }
  ]
}